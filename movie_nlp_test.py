# -*- coding: utf-8 -*-
"""Movie_NLP_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eTfiKpv_cMRvgWMaJrSpwkEATeZUzvgQ
"""

#importing the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.text import Tokenizer

from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing import sequence 

import glob
import os

#from google.colab import drive
#drive.mount('/content/gdrive')

#If run locally on system
test_pos_list = glob.glob('data/aclImdb/test/pos/*.txt')
test_neg_list = glob.glob('data/aclImdb/test/neg/*.txt')

reviews_test = []

for file_path in test_pos_list:
    with open(file_path, encoding="utf8") as f_input:
        reviews_test.append(f_input.read())

for file_path in test_neg_list:
    with open(file_path, encoding="utf8") as f_input:
        reviews_test.append(f_input.read())

# Load the train dataset into memory
#reviews_train = []
#for line in open('/content/gdrive/MyDrive/Colab Notebooks/657 Assignment 3/aclImdb/movie_data/full_train.txt', 'r'): 
  #reviews_train.append(line.strip())

#If run locally on system
train_pos_list = glob.glob('data/aclImdb/train/pos/*.txt')
train_neg_list = glob.glob('data/aclImdb/train/neg/*.txt')

reviews_train = []

for file_path in train_pos_list:
    with open(file_path, encoding="utf8") as f_input:
        reviews_train.append(f_input.read())

for file_path in train_neg_list:
    with open(file_path, encoding="utf8") as f_input:
        reviews_train.append(f_input.read())

# Use the default tokenizer settings  
tokenizer = tfds.deprecated.text.Tokenizer()
vocabulary_set = set()  
MAX_TOKENS = 0  
for example in reviews_train:  
  some_tokens = tokenizer.tokenize(example)  
  if MAX_TOKENS < len(some_tokens):  
            MAX_TOKENS = len(some_tokens)  
  vocabulary_set.update(some_tokens)

imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set,  
                                                   lowercase=True,  
                                                   tokenizer=tokenizer)  
vocab_size = imdb_encoder.vocab_size 
print(vocab_size, MAX_TOKENS)

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(reviews_train)

words_to_index = tokenizer.word_index

# Load the test dataset 
#reviews_test = []
#for line in open('/content/gdrive/MyDrive/Colab Notebooks/657 Assignment 3/aclImdb/movie_data/full_test.txt', 'r'): 
  #reviews_test.append(line.strip())

encoded = tokenizer.texts_to_sequences(reviews_test)

pad = sequence.pad_sequences(encoded, padding = 'post', maxlen = 150)
X_test = np.array(pad)
target = np.array([1 if i < 12500 else 0 for i in range(25000)])

#Load saved model
#model = keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/models/NLP_model') #For Colab

model = tf.keras.models.load_model('models/NLP_model') #Locally Run (Where model was saved)

model.summary()

model.evaluate(X_test, target)

