# -*- coding: utf-8 -*-
"""Movie_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n4C8cYRC0qFYOopKq5GoGEt4RvZ90ikN
"""

#importing the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from sklearn.model_selection import train_test_split
import tensorflow_datasets as tfds 

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow.keras import preprocessing
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from keras.models import Sequential
from keras.layers import Conv1D, InputLayer, Embedding, Bidirectional, LSTM, Dense, GRU, Dropout, BatchNormalization, MaxPooling2D, Flatten, MaxPooling1D, AveragePooling1D


from keras.preprocessing.text import text_to_word_sequence
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing import sequence  

import glob
import os

from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

from nltk.stem.porter import PorterStemmer

#from google.colab import drive
#drive.mount('/content/gdrive')

# Load the train dataset into memory
#reviews_train = []
#for line in open('/content/gdrive/MyDrive/Colab Notebooks/aclImdb/movie_data/full_train.txt', 'r'): 
  #reviews_train.append(line.strip())

#If run locally on system
train_pos_list = glob.glob('data/aclImdb/train/pos/*.txt')
train_neg_list = glob.glob('data/aclImdb/train/neg/*.txt')

reviews_train = []

for file_path in train_pos_list:
    with open(file_path, encoding="utf8") as f_input:
        reviews_train.append(f_input.read())

for file_path in train_neg_list:
    with open(file_path, encoding="utf8") as f_input:
        reviews_train.append(f_input.read()) '''

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(reviews_train)

words_to_index = tokenizer.word_index

encoded = tokenizer.texts_to_sequences(reviews_train)

pad = sequence.pad_sequences(encoded, padding = 'post', maxlen = 150)

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip

dict_w2v = {} 
with open('glove.6B.100d.txt', "r") as file:     
    for line in file:         
       tokens = line.split()      
       word = tokens[0]         
       vector = np.array(tokens[1:], dtype=np.float32) 
       if vector.shape[0] == 100:    
           dict_w2v[word] = vector 
       else:      
           print("There was an issue with " + word) 
# let's check the vocabulary size 
print("Dictionary Size: ", len(dict_w2v))

unk_cnt = 0 
unk_set = set() 
for word in imdb_encoder.tokens:   
    embedding_vector = dict_w2v.get(word) 
    if embedding_vector is not None:         
        tkn_id = imdb_encoder.encode(word)[0] 
        embedding_matrix[tkn_id] = embedding_vector 
    else:         
        unk_cnt += 1         
        unk_set.add(word) 
# Print how many weren't found 
print("Total unknown words: ", unk_cnt)

embeddings_index = {}
with open('glove.6B.100d.txt') as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

print("Found %s word vectors." % len(embeddings_index))

target = [1 if i < 12500 else 0 for i in range(25000)]

X_train, X_val, y_train, y_val = train_test_split(np.array(pad), np.array(target), stratify = target, random_state = 0, test_size = 0.2)

tf.random.set_seed(
    0
)

def read_glove_vector(glove_vec):
  with open(glove_vec, 'r', encoding='UTF-8') as f:
    words = set()
    word_to_vec_map = {}
    for line in f:
      w_line = line.split()
      curr_word = w_line[0]
      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)



  return word_to_vec_map

word_to_vec_map = read_glove_vector('glove.6B.50d.txt') #Directory where th

maxLen = 150

vocab_len = len(words_to_index)
embed_vector_len = word_to_vec_map['moon'].shape[0]

emb_matrix = np.zeros((vocab_len, embed_vector_len))

for word, index in words_to_index.items():
  embedding_vector = word_to_vec_map.get(word)
  if embedding_vector is not None:
    emb_matrix[index, :] = embedding_vector

model = Sequential() #BILSTM
model.add(InputLayer(input_shape=(150, )))
model.add(Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False))
model.add(Bidirectional(GRU(150, kernel_regularizer=tf.keras.regularizers.L1(0.01), return_sequences=True)))
model.add(Dropout(0.2))

model.add(Bidirectional(GRU(64, return_sequences = False)))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model_1 = model.fit(X_train, y_train, epochs= 15, batch_size= 128, validation_data= (X_val, y_val))

model.summary()

model2 = Sequential()
model2.add(InputLayer(input_shape=(150, )))
model2.add(Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False))
model2.add(Conv1D(64, kernel_size= 3, activation = 'relu'))
model2.add(AveragePooling1D(2))
model2.add(Dropout(0.2))
model2.add(Conv1D(64, padding= 'same', kernel_size= 3, activation = 'relu'))
model2.add(MaxPooling1D())
model2.add(Dropout(0.2))

model2.add(Flatten())
model2.add(Dense(128, activation = 'relu'))
model2.add(Dropout(0.2))

model2.add(Dense(1, activation= 'sigmoid'))

model2.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model2_history = model2.fit(X_train, y_train, epochs= 15, batch_size= 128, validation_data= (X_val, y_val))

"""# REPORT

Please note that google colab GPU was used.
1. Preprocessing steps: We used tensorflow dataset tokenizer which converts words to tokens and makes them lower cases. Using this we shortened each review to a maximum of 150 words and embedded these words using a glove embedding of 50 dimensions. The train dataset was split into equal samples of positive and negative reviews in ratio 80:20 by setting stratify of the sklearn library to true

2. Network Designs can be found in the model summaries. We compared an RNN model with a CNN model, both trained for 15 epochs and with a binary cross entropy loss function and adam optimizer. Both models had embedding layers.
The CNN model was our final model and would be described more.
We used 1D Convolutional layers because embeddings do not have color settings like rgb or black and white. We also added pooling layers to reduce dimensionality. These can be found in the model summaries below. ReLU activation functions were used.

3. Training and validation accuracies are shown in the plots below. We see the CNN model outperform the RNN model clearly

4. Comments on the output
To improve the model, perhaps weights initialization, early stopping and training for a reduced number of epochs might improve model generalization.
The CNN model also trained faster than the RNN model due to its smaller number of trainable parameters

We achieved a test accuracy of 83% on the test set
"""

model.summary()

model2.summary()

epochs = range(1,16)
models = [model_1, model2_history]
plt.figure(figsize = (12,8))
for i in models:
  plt.plot(epochs, i.history['accuracy'])
plt.title('Training Accuracy of Models')
plt.legend(['RNN Model', 'CNN Model'])
plt.xlabel('epochs')
plt.ylabel('Accuracy')

epochs = range(1,16)
models = [model_1, model2_history]
plt.figure(figsize = (12,8))
for i in models:
  plt.plot(epochs, i.history['val_accuracy'])
plt.title('Validation Accuracy of Models')
plt.legend(['RNN Model', 'CNN Model'])
plt.xlabel('epochs')
plt.ylabel('Accuracy')

model2.save('models/NLP_model') #Locally

